# Lapa LLM

## Представляємо Lapa LLM v0.1.2 — найефективнішу українську відкриту мовну модель 

25.10.2025

Сьогодні ми з гордістю представляємо Lapa LLM — передову відкриту велику мовну модель на основі Gemma-3-12B з фокусом на обробку української мови. Проєкт є результатом багатомісячної роботи команди українських дослідників у галузі штучного інтелекту з Українського Католицького Університету, AGH University of Krakow, КПІ ім. Ігоря Сікорського та Львівської Політехніки, які об'єдналися для створення найкращої моделі для обробки української мови.

Модель названо на честь [Валентина Лапи](https://de.wikipedia.org/wiki/Walentyn_Lapa), який разом з [Олексієм Івахненком](https://uk.wikipedia.org/wiki/%D0%86%D0%B2%D0%B0%D1%85%D0%BD%D0%B5%D0%BD%D0%BA%D0%BE_%D0%9E%D0%BB%D0%B5%D0%BA%D1%81%D1%96%D0%B9_%D0%93%D1%80%D0%B8%D0%B3%D0%BE%D1%80%D0%BE%D0%B2%D0%B8%D1%87) створив метод групового урахування аргументів, який є попередником Deep Learning [(джерело)](https://people.idsia.ch/~juergen/DeepLearning2July2014.pdf).
 
Метою проєкту є створення найкращої моделі для обробки української мови з відкритими датасетами для претрейнінгу та навчання інструкціям.

### Ключові досягнення

**Найкращий токенізатор для української мови**

Завдяки розробленому [Миколою Гальтюком](https://www.linkedin.com/in/mykola-haltiuk/) в рамках цього проєкту SOTA методу для адаптації токенайзера, вдалось замінити `80_000` токенів із `250_000` на українські без втрати якості моделі, таким чином зробивши Lapa LLM найшвидшою моделлю для роботи з українською мовою. У порівнянні зі оригінальною Gemma 3, для роботи із українською мовою модель потребує півтора раза менше токенів, таким чином роблячи утричі менше обчислень для досягнення кращого результату.

**Найефективніша instruction-tuned модель на ринку**

Наша instruction версія моделі у деяких категоріях на бенчмарках незначно поступається поточному лідеру — [MamayLM](https://huggingface.co/spaces/INSAIT-Institute/mamaylm-v1-blog). Команда активно працює над новими датасетами для подальшого покращення показників на бенчмарках, які плануємо перевершити у v1.0 моделі.

### Результати бенчмарків

- Найкращий перекладач з англійської на українську з результатом у 33 BLEU на FLORES та навпаки, що дозволяє натурально та дешево перекладати нові NLP датасети на українську
- Одна з кращих моделей для обробки зображень українською у своєму розмірі, що було заміряно на бенчмарку MMZNO
- Одна з найкращих моделей для Summarization та Q&A, що означає класну роботу для RAG
- Тести запитань на пропаганду та дезінформацію показують продуктивність підходу з фільтрації на етапі претрейнінгу та на етапі донавчання на інструкціях

Заміри моделей та порівняння будуть опубліковані в рамках проєкту Ukrainian LLM Leaderboard, підписуйтесь на телеграм-канал для подальших новин.


**Лідер за результатами претренінгу**

Lapa LLM демонструє найкращі показники у бенчмарках претренінгу для обробки української мови, що відкриває можливості для використання іншими дослідниками для адаптації під власні задачі.
  
Модель було натреновано на даних, оцінених різними моделями для оцінки якості - оцінки наявності пропаганди та дезінформації, читабельності, оцінки граматики тощо. На кінцевих етапах тренування модель тренувалась на високоякісних матеріалах, наданих для комерційного користування підрозділом відкритих даних Бібліотеки Гарварду.

**Максимальна відкритість та прозорість**

На відміну від більшості доступних моделей, Lapa LLM є максимально відкритим проєктом:
- Модель доступна для комерційного використання
- Опубліковано близько 25 датасетів для тренування моделі
- Розкрито методи фільтрації та обробки даних, в тому числі для визначення дезінформації та пропаганди
- Відкритий вихідний код моделі
- Доступна документація процесу тренування

Така відкритість дозволяє розвинути українську NLP спільноту та допомогти бізнесу отримати інструмент для найефективнішої обробки української мови як за обчисленнями, так і за результатами.


### Можливості застосування

Lapa LLM відкриває широкі можливості для:
- Обробки чутливих документів без передачі даних на зовнішні сервери
- Роботи з українськими текстами з урахуванням культурного та історичного контексту без code-switching на російську чи інші мови
- Для побудови RAG-систем та чатботів, що пишуть грамотною українською
- Розробки спеціалізованих рішень через можливість донавчання під конкретні завдання
- Машинного перекладу з найкращою якістю перекладу з англійської на українську і навпаки серед усіх моделей, в тому числі API провайдерів

### Наступні кроки

- Завершити розробку міркувальної моделі
- Ми збираємо відгуки спільноти про роботу моделі, тому чекаємо їх на GitHub або HuggingFace!
- Збираємо додаткові датасети для обробки зображень українською
- Збираємо додаткові датасети на слідування інструкціям та програмування

### Подяка спонсорам

Створення Lapa LLM стало можливим завдяки підтримці наших партнерів та спонсорів, в першу чергу стартапу **Comand.AI**, який надав обчислювальні ресурси для навчання моделі. Також хочемо подякувати компанії **ELEKS**, яка підтримала цей проєкт через грант, присвячений памʼяті Олексія Скрипника та стартапу **HuggingFace**, який надав безкоштовну корпоративну підписку на команду для зберігання моделей та датасетів/


### Посилання:

Спробувати модель: https://huggingface.co/spaces/lapa-llm/lapa  
Код: https://github.com/lapa-llm/lapa-llm 


Підписуйтесь на телеграм канал для подальших новин про проєкт: https://t.me/pehade_blog 



### Команда
